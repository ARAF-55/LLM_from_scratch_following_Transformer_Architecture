{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "block_size = 8\n",
    "batch_size = 4\n",
    "max_iters = 1000\n",
    "learning_rate = 3e-4\n",
    "eval_iters = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '!', '\"', '&', \"'\", '(', ')', '*', ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '\\ufeff']\n",
      "81\n"
     ]
    }
   ],
   "source": [
    "with open(\"wizard_of_oz.txt\", 'r', encoding = 'utf8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "chars = sorted(set(text))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "print(chars)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([80,  1,  1, 28, 39, 42, 39, 44, 32, 49])\n"
     ]
    }
   ],
   "source": [
    "string_to_int = {ch:i for i, ch in enumerate(chars)}\n",
    "int_to_strings = {i:ch for i, ch in enumerate(chars)}\n",
    "\n",
    "encode = lambda x: [string_to_int[c] for c in x]\n",
    "decode = lambda n: ''.join([int_to_strings[l] for l in n])\n",
    "\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "\n",
    "print(data[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs: \n",
      "tensor([[61,  1, 60, 65, 54, 72, 72,  1],\n",
      "        [61, 58, 78,  1, 71, 58, 54, 56],\n",
      "        [69,  0, 74, 72,  9,  3,  1, 72],\n",
      "        [68, 59,  1, 73, 61, 58,  1, 37]])\n",
      "torch.Size([4, 8])\n",
      "targets: \n",
      "tensor([[ 1, 60, 65, 54, 72, 72,  1, 55],\n",
      "        [58, 78,  1, 71, 58, 54, 56, 61],\n",
      "        [ 0, 74, 72,  9,  3,  1, 72, 74],\n",
      "        [59,  1, 73, 61, 58,  1, 37, 54]])\n",
      "torch.Size([4, 8])\n"
     ]
    }
   ],
   "source": [
    "n = int(0.8*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    \n",
    "    ix = torch.randint((len(data) - block_size), (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+1+block_size] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    \n",
    "    return x, y\n",
    "\n",
    "x, y = get_batch('train')\n",
    "\n",
    "print('inputs: ')\n",
    "print(x)\n",
    "print(x.shape)\n",
    "\n",
    "print('targets: ')\n",
    "print(y)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def estimate_loss(model):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    \n",
    "    for split in ['train', 'test']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        \n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    \n",
    "    model.train()\n",
    "    return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "w[\n",
      "(PgtoLme_﻿7pwWQ5t,\n",
      ":&MK,\n",
      "ZlFZ﻿)91ZWF2BK 1sx_OMK)zRbQEFunS6Pv?F28cu1eK:4Mwo(6(jm3Vf[A(S6V8K;uZkKZb.Z1riq?))z&P],??LMlK6Onyn2S7f)B [zGw)&o3cH;f3b5XT3sBBshD4v0pCJr4i,U_O\n",
      "wSIj)rRGWE3rjbd0(N,pB i,st!oBK5uGWyJ-Jb.vvCGs,Q.ZV8-p\n",
      "\"EGLMx.CL*\".3c.-IQf?KnAV*\"MhBK5Hy[FxbcAn1iZkw,_h(9ZE:UDizB?﻿Jc*Ce1*x;﻿o:A Iqu.Pv?mJj39\n",
      "nq?hFc*RaWAgm0*QfAVy1'qOn2NJ﻿y[ipjNK)3J0]qXYI4_8&rB&?c\n",
      ":dc\n",
      "([0d,AT'xTgt(X2H9y(TxYR0tP&&efM\n",
      "WCUa)y\"XE2gPu2mQ5 W\n",
      "\n",
      "a)pNbAgj_\n",
      "\"'QqN1iQGXd RP .iUi,\n",
      "It6?51Z*9Mjfna!.LXd;-Jt.izYD nJlH9JxHw﻿la-LWt!\n"
     ]
    }
   ],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "        \n",
    "    def forward(self, index, targets=None):\n",
    "        logits = self.token_embedding_table(index)\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = functional.cross_entropy(logits, targets)\n",
    "        \n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, index, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            \n",
    "            logits , loss = self.forward(index)\n",
    "            logits = logits[:,-1,:]\n",
    "            \n",
    "            probs = functional.softmax(logits, dim=-1)\n",
    "            index_next = torch.multinomial(probs, num_samples=1)\n",
    "            \n",
    "            index = torch.cat((index, index_next), dim=1)\n",
    "            \n",
    "        return index\n",
    "    \n",
    "model = BigramLanguageModel(vocab_size)\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "generated_chars = decode(model.generate(context, 500)[0].tolist())\n",
    "print(generated_chars)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, train loss: 4.133, val loss: 4.114\n",
      "step: 250, train loss: 4.103, val loss: 4.083\n",
      "step: 500, train loss: 4.013, val loss: 4.018\n",
      "step: 750, train loss: 3.961, val loss: 3.981\n",
      "step: 1000, train loss: 3.937, val loss: 3.915\n",
      "step: 1250, train loss: 3.890, val loss: 3.882\n",
      "step: 1500, train loss: 3.844, val loss: 3.856\n",
      "step: 1750, train loss: 3.802, val loss: 3.796\n",
      "step: 2000, train loss: 3.756, val loss: 3.741\n",
      "step: 2250, train loss: 3.713, val loss: 3.700\n",
      "step: 2500, train loss: 3.673, val loss: 3.666\n",
      "step: 2750, train loss: 3.642, val loss: 3.644\n",
      "step: 3000, train loss: 3.586, val loss: 3.605\n",
      "step: 3250, train loss: 3.567, val loss: 3.565\n",
      "step: 3500, train loss: 3.519, val loss: 3.528\n",
      "step: 3750, train loss: 3.504, val loss: 3.496\n",
      "step: 4000, train loss: 3.482, val loss: 3.451\n",
      "step: 4250, train loss: 3.449, val loss: 3.447\n",
      "step: 4500, train loss: 3.409, val loss: 3.389\n",
      "step: 4750, train loss: 3.351, val loss: 3.361\n",
      "step: 5000, train loss: 3.330, val loss: 3.335\n",
      "step: 5250, train loss: 3.302, val loss: 3.322\n",
      "step: 5500, train loss: 3.300, val loss: 3.304\n",
      "step: 5750, train loss: 3.260, val loss: 3.243\n",
      "step: 6000, train loss: 3.210, val loss: 3.246\n",
      "step: 6250, train loss: 3.197, val loss: 3.217\n",
      "step: 6500, train loss: 3.195, val loss: 3.195\n",
      "step: 6750, train loss: 3.144, val loss: 3.177\n",
      "step: 7000, train loss: 3.116, val loss: 3.148\n",
      "step: 7250, train loss: 3.107, val loss: 3.099\n",
      "step: 7500, train loss: 3.097, val loss: 3.116\n",
      "step: 7750, train loss: 3.087, val loss: 3.100\n",
      "step: 8000, train loss: 3.035, val loss: 3.088\n",
      "step: 8250, train loss: 3.010, val loss: 3.029\n",
      "step: 8500, train loss: 3.007, val loss: 3.019\n",
      "step: 8750, train loss: 3.007, val loss: 2.982\n",
      "step: 9000, train loss: 2.978, val loss: 2.983\n",
      "step: 9250, train loss: 2.955, val loss: 2.966\n",
      "step: 9500, train loss: 2.963, val loss: 2.963\n",
      "step: 9750, train loss: 2.898, val loss: 2.956\n",
      "step: 10000, train loss: 2.912, val loss: 2.930\n",
      "step: 10250, train loss: 2.926, val loss: 2.919\n",
      "step: 10500, train loss: 2.855, val loss: 2.917\n",
      "step: 10750, train loss: 2.852, val loss: 2.874\n",
      "step: 11000, train loss: 2.842, val loss: 2.877\n",
      "step: 11250, train loss: 2.847, val loss: 2.869\n",
      "step: 11500, train loss: 2.840, val loss: 2.864\n",
      "step: 11750, train loss: 2.797, val loss: 2.846\n",
      "step: 12000, train loss: 2.836, val loss: 2.840\n",
      "step: 12250, train loss: 2.784, val loss: 2.806\n",
      "step: 12500, train loss: 2.790, val loss: 2.786\n",
      "step: 12750, train loss: 2.775, val loss: 2.803\n",
      "step: 13000, train loss: 2.794, val loss: 2.805\n",
      "step: 13250, train loss: 2.791, val loss: 2.775\n",
      "step: 13500, train loss: 2.750, val loss: 2.754\n",
      "step: 13750, train loss: 2.684, val loss: 2.766\n",
      "step: 14000, train loss: 2.712, val loss: 2.777\n",
      "step: 14250, train loss: 2.741, val loss: 2.749\n",
      "step: 14500, train loss: 2.716, val loss: 2.734\n",
      "step: 14750, train loss: 2.717, val loss: 2.711\n",
      "step: 15000, train loss: 2.716, val loss: 2.752\n",
      "step: 15250, train loss: 2.702, val loss: 2.722\n",
      "step: 15500, train loss: 2.676, val loss: 2.705\n",
      "step: 15750, train loss: 2.670, val loss: 2.690\n",
      "step: 16000, train loss: 2.685, val loss: 2.671\n",
      "step: 16250, train loss: 2.690, val loss: 2.683\n",
      "step: 16500, train loss: 2.648, val loss: 2.695\n",
      "step: 16750, train loss: 2.641, val loss: 2.696\n",
      "step: 17000, train loss: 2.636, val loss: 2.666\n",
      "step: 17250, train loss: 2.640, val loss: 2.646\n",
      "step: 17500, train loss: 2.649, val loss: 2.679\n",
      "step: 17750, train loss: 2.641, val loss: 2.646\n",
      "step: 18000, train loss: 2.615, val loss: 2.669\n",
      "step: 18250, train loss: 2.610, val loss: 2.660\n",
      "step: 18500, train loss: 2.619, val loss: 2.647\n",
      "step: 18750, train loss: 2.601, val loss: 2.643\n",
      "step: 19000, train loss: 2.626, val loss: 2.639\n",
      "step: 19250, train loss: 2.620, val loss: 2.626\n",
      "step: 19500, train loss: 2.590, val loss: 2.634\n",
      "step: 19750, train loss: 2.603, val loss: 2.644\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(20000):\n",
    "    if iter % eval_iters == 0:\n",
    "        losses = estimate_loss(model)\n",
    "        print(f\"step: {iter}, train loss: {losses['train']:.3f}, val loss: {losses['test']:.3f}\")\n",
    "        \n",
    "    x, y  = get_batch('train')\n",
    "    logits, loss = model.forward(x, y)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
